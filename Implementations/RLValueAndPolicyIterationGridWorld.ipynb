{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipython-cache in /home/renato/anaconda3/envs/python36/lib/python3.6/site-packages (0.2.6)\n",
      "Requirement already satisfied: astunparse in /home/renato/anaconda3/envs/python36/lib/python3.6/site-packages (from ipython-cache) (1.6.2)\n",
      "Requirement already satisfied: tabulate in /home/renato/anaconda3/envs/python36/lib/python3.6/site-packages (from ipython-cache) (0.8.3)\n",
      "Requirement already satisfied: IPython in /home/renato/anaconda3/envs/python36/lib/python3.6/site-packages (from ipython-cache) (7.3.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/renato/anaconda3/envs/python36/lib/python3.6/site-packages (from astunparse->ipython-cache) (0.32.3)\n",
      "Requirement already satisfied: six<2.0,>=1.6.1 in /home/renato/anaconda3/envs/python36/lib/python3.6/site-packages (from astunparse->ipython-cache) (1.12.0)\n",
      "Requirement already satisfied: traitlets>=4.2 in /home/renato/anaconda3/envs/python36/lib/python3.6/site-packages (from IPython->ipython-cache) (4.3.2)\n",
      "Requirement already satisfied: decorator in /home/renato/anaconda3/envs/python36/lib/python3.6/site-packages (from IPython->ipython-cache) (4.3.2)\n",
      "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /home/renato/anaconda3/envs/python36/lib/python3.6/site-packages (from IPython->ipython-cache) (2.0.9)\n",
      "Requirement already satisfied: backcall in /home/renato/anaconda3/envs/python36/lib/python3.6/site-packages (from IPython->ipython-cache) (0.1.0)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /home/renato/anaconda3/envs/python36/lib/python3.6/site-packages (from IPython->ipython-cache) (4.6.0)\n",
      "Requirement already satisfied: jedi>=0.10 in /home/renato/anaconda3/envs/python36/lib/python3.6/site-packages (from IPython->ipython-cache) (0.13.3)\n",
      "Requirement already satisfied: setuptools>=18.5 in /home/renato/anaconda3/envs/python36/lib/python3.6/site-packages (from IPython->ipython-cache) (40.8.0)\n",
      "Requirement already satisfied: pickleshare in /home/renato/anaconda3/envs/python36/lib/python3.6/site-packages (from IPython->ipython-cache) (0.7.5)\n",
      "Requirement already satisfied: pygments in /home/renato/anaconda3/envs/python36/lib/python3.6/site-packages (from IPython->ipython-cache) (2.3.1)\n",
      "Requirement already satisfied: ipython-genutils in /home/renato/anaconda3/envs/python36/lib/python3.6/site-packages (from traitlets>=4.2->IPython->ipython-cache) (0.2.0)\n",
      "Requirement already satisfied: wcwidth in /home/renato/anaconda3/envs/python36/lib/python3.6/site-packages (from prompt-toolkit<2.1.0,>=2.0.0->IPython->ipython-cache) (0.1.7)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/renato/anaconda3/envs/python36/lib/python3.6/site-packages (from pexpect; sys_platform != \"win32\"->IPython->ipython-cache) (0.6.0)\n",
      "Requirement already satisfied: parso>=0.3.0 in /home/renato/anaconda3/envs/python36/lib/python3.6/site-packages (from jedi>=0.10->IPython->ipython-cache) (0.3.4)\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "\n",
    "!pip install ipython-cache\n",
    "import cache_magic\n",
    "from random import randint\n",
    "import sys\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7ACJ4WcqAdRg"
   },
   "source": [
    "# MDP Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9iTAO20cOV7z"
   },
   "outputs": [],
   "source": [
    "# Creating MDP representation for the grid world and organizing its functionalities\n",
    "\n",
    "class ActionResult:\n",
    "    def __init__ (self, resultState, prob, reward):\n",
    "        self.resultState = resultState\n",
    "        self.prob = prob\n",
    "        self.reward = reward\n",
    "        \n",
    "    def __str__(self):\n",
    "        return str(self.resultState) + \" \" + str(self.prob) + \" \" + str(self.reward)\n",
    "\n",
    "\n",
    "class Action:\n",
    "    # abstraction for an action. Expects a state on which this action\n",
    "    # can be taken and a list of ActionResult\n",
    "    def __init__ (self, state, results, label):\n",
    "        self.state = state\n",
    "        self.results = results\n",
    "        self.label = label\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.state) + \" \" + str([str(r) for r in self.results])\n",
    "    \n",
    "class State:\n",
    "    def __init__(self, id, actions, value):\n",
    "        self.id = id\n",
    "        self.actions = actions\n",
    "        self.value = value       \n",
    "\n",
    "        def __str__(self):\n",
    "            return str(self.id) + \" \" + str(self.value) + \" \" + str(self.actions)\n",
    "\n",
    "class DeterministicGridWorldMDP(object):\n",
    "    # creates a gridworld representation where the reward for every transition\n",
    "    # is given in the rewards matrix which means the reward the agent gets when achieving that reward\n",
    "    def __init__ (self, rewards, rows, cols):\n",
    "        self.transitions = [(0, 1), (0, -1), (1, 0), (-1, 0)]\n",
    "        self.nTransitionsPerState = len(self.transitions)\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "        self.rewards = rewards\n",
    "        self.policy = [[randint(0, self.nTransitionsPerState-1) for j in range(cols)] for i in range(rows)]\n",
    "        self.value = [[0 for j in range(cols)] for i in range(rows)]\n",
    "        self.states = [[State((i, j), self.generateActions(i, j), 0) for j in range(cols)] for i in range(rows)]\n",
    "    \n",
    "    def generateActions(self, i, j):\n",
    "        actions = []\n",
    "        prettyDirs = ['r', 'l', 'd', 'u']\n",
    "        for transition in range(self.nTransitionsPerState):\n",
    "            resultState = (i + self.transitions[transition][0], j + self.transitions[transition][1])\n",
    "            if self.isValidTransition(resultState):\n",
    "                actions.append(Action((i, j), [ActionResult(resultState, 1, self.rewards[resultState[0]][resultState[1]])], prettyDirs[transition]))\n",
    "         \n",
    "        return actions\n",
    "    \n",
    "    def getReward(self, dest):\n",
    "        return self.rewards[dest[0]][dest[1]]\n",
    "\n",
    "    def getStateValue(self, state):\n",
    "        return self.states[state[0]][state[1]].value\n",
    "\n",
    "    def isValidTransition(self, dest):\n",
    "        return  not ((dest[0] < 0 or dest[0] >= self.rows) or (dest[1] < 0 or dest[1] >= self.cols))\n",
    "    \n",
    "    def upadateStateValue(self, state):\n",
    "        maxV = float(\"-inf\")\n",
    "        actions = self.states[state[0]][state[1]].actions\n",
    "        for i in range(len(actions)):\n",
    "            action = actions[i]\n",
    "            resultState = action.results[0].resultState\n",
    "            r = action.results[0].reward\n",
    "            v = self.getStateValue(resultState)\n",
    "            if r + v > maxV:\n",
    "                maxV = r + v\n",
    "                self.policy[state[0]][state[1]] = action.label\n",
    "                self.states[state[0]][state[1]].value = maxV\n",
    "    \n",
    "    def printValues(self):\n",
    "        for r in self.value:\n",
    "            print(r)\n",
    "            \n",
    "    def getPolicy(self):\n",
    "        return self.policy\n",
    "        \n",
    "        \n",
    "    def printPolicy(self):\n",
    "        for r in self.policy:\n",
    "            print(r)\n",
    "\n",
    "    def iterateValues(self):\n",
    "        for i in range(self.rows):\n",
    "            for j in range(self.cols):\n",
    "                self.upadateStateValue((i, j))\n",
    "\n",
    "    def iterateValuesUntilConverge(self, printSteps=False):\n",
    "        while True:\n",
    "            values = self.value\n",
    "            self.iterateValues()\n",
    "            if printSteps:\n",
    "                print()\n",
    "                self.printPolicy()      \n",
    "            if values == self.value:\n",
    "                break\n",
    "                \n",
    "    def repeatIterateValues(self, n, printSteps=False):\n",
    "        for i in range(n):\n",
    "            self.iterateValues()\n",
    "            if printSteps:\n",
    "                print()\n",
    "                self.printPolicy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Q0jChHJAwYW"
   },
   "source": [
    "# Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vrzTI84pA091"
   },
   "outputs": [],
   "source": [
    "def printMatrix(matrix):\n",
    "    for r in matrix:\n",
    "        print(r)\n",
    "    print()\n",
    "\n",
    "def generateRandomRewards(row, cols, goalReward, notGoalReward):\n",
    "    goalCoordX, goalCoordY = randint(0, rows - 1), randint(0, cols - 1)\n",
    "    rewards = [[notGoalReward for j in range(cols)] for i in range(rows)]\n",
    "    rewards[goalCoordX][goalCoordY] = goalReward\n",
    "    return rewards, (goalCoordX, goalCoordY)\n",
    "\n",
    "def generateRandomRewardsWithObstacles(row, cols, goalReward, notGoalReward, obstacleReward, nObstacles):\n",
    "    \n",
    "    rewards = [[notGoalReward for j in range(cols)] for i in range(rows)]\n",
    "    \n",
    "    goalCoordX, goalCoordY = randint(0, rows - 1), randint(0, cols - 1)\n",
    "    goal = (goalCoordX, goalCoordY)\n",
    "    rewards[goalCoordX][goalCoordY] = goalReward\n",
    "    \n",
    "    obstacles = []\n",
    "    while nObstacles > 0:\n",
    "        obs = randint(0, rows - 1), randint(0, cols - 1)\n",
    "        if not obs in obstacles and not obs == goal:\n",
    "            obstacles.append(obs)\n",
    "            nObstacles = nObstacles - 1\n",
    "            rewards[obs[0]][obs[1]] = obstacleReward\n",
    "    \n",
    "    return rewards, goal, obstacles\n",
    "\n",
    "\n",
    "def markGoalAndObstaclesOnGrid(grid, goal, obstacles):\n",
    "    grid[goal[0]][goal[1]] = 'G'\n",
    "    for o in obstacles:\n",
    "        grid[o[0]][o[1]] = 'X'\n",
    "    \n",
    "    return grid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "evLPPBUGAmRU"
   },
   "source": [
    "# Tests with deterministics gridworld environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2414
    },
    "colab_type": "code",
    "id": "SL4ajJPnOhZU",
    "outputId": "5dbdce19-efd8-4058-e620-13c0072a9a25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 0, 0]\n",
      "[3, 3, 1]\n",
      "[3, 0, 3]\n",
      "\n",
      "['r', 'd', 'l']\n",
      "['r', 'G', 'l']\n",
      "['r', 'u', 'l']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First dummy smoke test to make sure everything is not absurdly wrong\n",
    "\n",
    "rewards = [\n",
    "    [-1, -1, -1],\n",
    "    [-1, 100, -1],\n",
    "    [-1, -1, -1]\n",
    "]\n",
    "\n",
    "rows, cols = 3, 3\n",
    "\n",
    "mdp = DeterministicGridWorldMDP(rewards, rows, cols)\n",
    "mdp.printPolicy()\n",
    "mdp.repeatIterateValues(10)\n",
    "print ()\n",
    "printMatrix(markGoalAndObstaclesOnGrid(mdp.getPolicy(), (1,1), []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2309
    },
    "colab_type": "code",
    "id": "LXzCBKj-O0pi",
    "outputId": "b109b700-c9eb-4c3e-d434-caf18acd7920"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1, 0, 1, 2, 2, 2, 3, 2, 1]\n",
      "[0, 3, 0, 2, 0, 1, 2, 2, 0, 3]\n",
      "[3, 2, 1, 3, 0, 3, 3, 2, 1, 2]\n",
      "[1, 1, 2, 2, 1, 0, 1, 2, 2, 3]\n",
      "[0, 3, 0, 0, 1, 3, 3, 3, 0, 3]\n",
      "[2, 0, 0, 3, 0, 1, 1, 1, 0, 2]\n",
      "[2, 3, 3, 0, 2, 1, 1, 3, 0, 3]\n",
      "[0, 3, 3, 1, 0, 0, 1, 3, 1, 1]\n",
      "[3, 2, 1, 0, 1, 0, 3, 1, 2, 1]\n",
      "[0, 3, 2, 2, 0, 0, 1, 0, 3, 0]\n",
      "['r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'd', 'l']\n",
      "['r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'd', 'l']\n",
      "['r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'd', 'l']\n",
      "['r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'G', 'l']\n",
      "['r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'u', 'l']\n",
      "['r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'u', 'l']\n",
      "['r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'u', 'l']\n",
      "['r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'u', 'l']\n",
      "['r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'u', 'l']\n",
      "['r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'u', 'l']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Testing random test case for 10 rows and cols  \n",
    "\n",
    "rows, cols = 10, 10\n",
    "\n",
    "rewards, goal = generateRandomRewards(rows, cols, 100, 0)\n",
    "mdp = DeterministicGridWorldMDP(rewards, rows, cols)\n",
    "mdp.printPolicy()\n",
    "mdp.repeatIterateValues(10)\n",
    "printMatrix(markGoalAndObstaclesOnGrid(mdp.getPolicy(), goal, []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 833
    },
    "colab_type": "code",
    "id": "3Dyfhj1iRlY5",
    "outputId": "c633b0ff-eb8b-44e8-a5da-9a3f3382a9c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards\n",
      "[0, 0, 0, 0, -1000]\n",
      "[0, 0, 0, 0, 100]\n",
      "[-1000, 0, 0, -1000, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, -1000, -1000, 0]\n",
      "\n",
      "goal (1, 4)\n",
      "obstacles [(4, 3), (0, 4), (2, 0), (4, 2), (2, 3)]\n",
      "\n",
      "[3, 2, 3, 0, 3]\n",
      "[0, 1, 3, 0, 1]\n",
      "[3, 0, 0, 1, 1]\n",
      "[2, 0, 3, 0, 2]\n",
      "[0, 2, 2, 2, 1]\n",
      "\n",
      "['r', 'r', 'r', 'd', 'X']\n",
      "['r', 'r', 'r', 'r', 'G']\n",
      "['X', 'r', 'u', 'X', 'u']\n",
      "['r', 'r', 'r', 'r', 'u']\n",
      "['r', 'u', 'X', 'X', 'u']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Grid world with obstacles (things are getting interesting)\n",
    "\n",
    "rows, cols = 5, 5\n",
    "\n",
    "rewards, goal, obs = generateRandomRewardsWithObstacles(rows, cols, 100, 0, -1000, 5)\n",
    "print (\"rewards\")\n",
    "printMatrix(rewards)\n",
    "print (\"goal\", goal)\n",
    "print (\"obstacles\", obs)\n",
    "print()\n",
    "\n",
    "mdp = DeterministicGridWorldMDP(rewards, rows, cols)\n",
    "mdp.printPolicy()\n",
    "mdp.repeatIterateValues(10)\n",
    "print()\n",
    "\n",
    "printMatrix(markGoalAndObstaclesOnGrid(mdp.getPolicy(), goal, obs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 5175
    },
    "colab_type": "code",
    "id": "FYJZ2xzpaUYw",
    "outputId": "39c2bbaa-1253-45d7-8972-fd468744b7f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards\n",
      "[0, 0, 0, -1000, 0]\n",
      "[0, -1000, 0, 0, -1000]\n",
      "[-1000, 0, 0, 0, 0]\n",
      "[0, -1000, 100, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "\n",
      "goal (3, 2)\n",
      "obstacles [(1, 4), (3, 1), (2, 0), (1, 1), (0, 3)]\n",
      "\n",
      "['r', 'r', 'l', 'X', 'l']\n",
      "['u', 'X', 'r', 'l', 'X']\n",
      "['X', 'r', 'd', 'l', 'l']\n",
      "['d', 'X', 'G', 'l', 'l']\n",
      "['r', 'r', 'u', 'l', 'l']\n",
      "\n",
      "['r', 'r', 'd', 'X', 'l']\n",
      "['u', 'X', 'd', 'l', 'X']\n",
      "['X', 'r', 'd', 'l', 'l']\n",
      "['d', 'X', 'G', 'l', 'l']\n",
      "['r', 'r', 'u', 'l', 'l']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This case proves my iterate until converge is not working propperly\n",
    "\n",
    "rewards = [\n",
    "    [0, 0, 0, -1000, 0],\n",
    "    [0, -1000, 0, 0, -1000],\n",
    "    [-1000, 0, 0, 0, 0],\n",
    "    [0, -1000, 100, 0, 0],\n",
    "    [0, 0, 0, 0, 0]\n",
    "]\n",
    "\n",
    "goal = (3, 2)\n",
    "obs = [(1, 4), (3, 1), (2, 0), (1, 1), (0, 3)]\n",
    "print (\"rewards\")\n",
    "printMatrix(rewards)\n",
    "print (\"goal\", goal)\n",
    "print (\"obstacles\", obs)\n",
    "print()\n",
    "\n",
    "mdp = DeterministicGridWorldMDP(rewards, rows, cols)\n",
    "mdp.iterateValuesUntilConverge()\n",
    "\n",
    "printMatrix(markGoalAndObstaclesOnGrid(mdp.getPolicy(), goal, obs))\n",
    "\n",
    "mdp = DeterministicGridWorldMDP(rewards, rows, cols)\n",
    "mdp.repeatIterateValues(10)\n",
    "printMatrix(markGoalAndObstaclesOnGrid(mdp.getPolicy(), goal, obs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 5175
    },
    "colab_type": "code",
    "id": "41V_R0MyZfEO",
    "outputId": "e342fdf7-a7c3-4baa-92e5-e5bd4d81d84c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, -1000, 0, -1000]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, -1000, -1000, -1000, -1000]\n",
      "[0, 0, 0, 0, 100]\n",
      "\n",
      "goal (4, 4)\n",
      "obstacles [(3, 1), (3, 2), (3, 3), (3, 4), (1, 2), (1, 4)]\n",
      "\n",
      "['r', 'r', 'r', 'r', 'l']\n",
      "['r', 'l', 'X', 'd', 'X']\n",
      "['r', 'r', 'r', 'r', 'l']\n",
      "['d', 'X', 'X', 'X', 'X']\n",
      "['r', 'r', 'r', 'r', 'G']\n",
      "\n",
      "['d', 'l', 'l', 'l', 'l']\n",
      "['d', 'l', 'X', 'd', 'X']\n",
      "['d', 'l', 'l', 'l', 'l']\n",
      "['d', 'X', 'X', 'X', 'X']\n",
      "['r', 'r', 'r', 'r', 'G']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Another case where iterating until converge do not work\n",
    "\n",
    "rewards = [\n",
    "    [0,     0,      0,     0, 0],\n",
    "    [0,     0,     -1000,  0, -1000],\n",
    "    [0,     0,     0,      0, 0],\n",
    "    [0,     -1000, -1000,  -1000, -1000],\n",
    "    [0,     0,     0,    0, 100]\n",
    "]\n",
    "\n",
    "rows, cols = 5, 5\n",
    "\n",
    "goal = (4, 4)\n",
    "obs = [(3, 1),(3, 2), (3, 3), (3, 4), (1, 2), (1, 4)]\n",
    "print (\"rewards\")\n",
    "printMatrix(rewards)\n",
    "print (\"goal\", goal)\n",
    "print (\"obstacles\", obs)\n",
    "print()\n",
    "\n",
    "mdp = DeterministicGridWorldMDP(rewards, rows, cols)\n",
    "mdp.iterateValuesUntilConverge()\n",
    "printMatrix(markGoalAndObstaclesOnGrid(mdp.getPolicy(), goal, obs))\n",
    "\n",
    "mdp = DeterministicGridWorldMDP(rewards, rows, cols)\n",
    "mdp.repeatIterateValues(10)\n",
    "printMatrix(markGoalAndObstaclesOnGrid(mdp.getPolicy(), goal, obs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 5175
    },
    "colab_type": "code",
    "id": "9ltOyA_ef6vL",
    "outputId": "b182074a-6cc5-4a17-8c59-2eae7a183bba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, -1000, 0, -1000]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, -250, -500, -750, -1000]\n",
      "[0, 0, 0, 0, 100]\n",
      "\n",
      "goal (4, 4)\n",
      "obstacles [(3, 1), (3, 2), (3, 3), (3, 4), (1, 2), (1, 4)]\n",
      "\n",
      "['r', 'r', 'r', 'r', 'l']\n",
      "['r', 'l', 'X', 'd', 'X']\n",
      "['r', 'r', 'r', 'r', 'l']\n",
      "['d', 'X', 'X', 'X', 'X']\n",
      "['r', 'r', 'r', 'r', 'G']\n",
      "\n",
      "['d', 'l', 'l', 'l', 'l']\n",
      "['d', 'l', 'X', 'd', 'X']\n",
      "['d', 'l', 'l', 'l', 'l']\n",
      "['d', 'X', 'X', 'X', 'X']\n",
      "['r', 'r', 'r', 'r', 'G']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lets have variable obstacles with variable penalties\n",
    "\n",
    "rewards = [\n",
    "    [0,     0,      0,     0, 0],\n",
    "    [0,     0,     -1000,  0, -1000],\n",
    "    [0,     0,     0,      0, 0],\n",
    "    [0,     -250, -500,  -750, -1000],\n",
    "    [0,     0,     0,    0, 100]\n",
    "]\n",
    "\n",
    "rows, cols = 5, 5\n",
    "\n",
    "goal = (4, 4)\n",
    "obs = [(3, 1),(3, 2), (3, 3), (3, 4), (1, 2), (1, 4)]\n",
    "print (\"rewards\")\n",
    "printMatrix(rewards)\n",
    "print (\"goal\", goal)\n",
    "print (\"obstacles\", obs)\n",
    "print()\n",
    "\n",
    "mdp = DeterministicGridWorldMDP(rewards, rows, cols)\n",
    "mdp.iterateValuesUntilConverge()\n",
    "printMatrix(markGoalAndObstaclesOnGrid(mdp.getPolicy(), goal, obs))\n",
    "\n",
    "mdp = DeterministicGridWorldMDP(rewards, rows, cols)\n",
    "mdp.repeatIterateValues(10)\n",
    "printMatrix(markGoalAndObstaclesOnGrid(mdp.getPolicy(), goal, obs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 9517
    },
    "colab_type": "code",
    "id": "J_rzSqXrgxKa",
    "outputId": "0e74447f-8465-48b7-ae49-8621d08b7eaf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards\n",
      "[-100, -100, -100, -100, -10]\n",
      "[-100, -100, -1000, -100, -1000]\n",
      "[-100, -100, -100, -100, -10]\n",
      "[-100, -250, -500, -750, -1000]\n",
      "[-100, -100, -100, -100, 100]\n",
      "\n",
      "goal (4, 4)\n",
      "obstacles [(3, 1), (3, 2), (3, 3), (3, 4), (1, 2), (1, 4)]\n",
      "\n",
      "['r', 'r', 'r', 'r', 'l']\n",
      "['r', 'd', 'X', 'd', 'X']\n",
      "['r', 'r', 'r', 'r', 'l']\n",
      "['d', 'X', 'X', 'X', 'X']\n",
      "['r', 'r', 'r', 'r', 'G']\n",
      "\n",
      "['d', 'd', 'l', 'l', 'l']\n",
      "['d', 'd', 'X', 'd', 'X']\n",
      "['d', 'd', 'l', 'l', 'l']\n",
      "['d', 'X', 'X', 'X', 'X']\n",
      "['r', 'r', 'r', 'r', 'G']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lets now give negative reward for moving\n",
    "\n",
    "rewards = [\n",
    "    [-100,     -100,      -100,     -100, -10],\n",
    "    [-100,     -100,     -1000,  -100, -1000],\n",
    "    [-100,     -100,     -100,      -100, -10],\n",
    "    [-100,     -250, -500,  -750, -1000],\n",
    "    [-100,     -100,     -100,    -100, 100]\n",
    "]\n",
    "\n",
    "rows, cols = 5, 5\n",
    "\n",
    "goal = (4, 4)\n",
    "obs = [(3, 1),(3, 2), (3, 3), (3, 4), (1, 2), (1, 4)]\n",
    "print (\"rewards\")\n",
    "printMatrix(rewards)\n",
    "print (\"goal\", goal)\n",
    "print (\"obstacles\", obs)\n",
    "print()\n",
    "\n",
    "mdp = DeterministicGridWorldMDP(rewards, rows, cols)\n",
    "mdp.iterateValuesUntilConverge()\n",
    "printMatrix(markGoalAndObstaclesOnGrid(mdp.getPolicy(), goal, obs))\n",
    "\n",
    "mdp = DeterministicGridWorldMDP(rewards, rows, cols)\n",
    "mdp.repeatIterateValues(20)\n",
    "printMatrix(markGoalAndObstaclesOnGrid(mdp.getPolicy(), goal, obs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 4889258
    },
    "colab_type": "code",
    "id": "Fexcl8xdd7LW",
    "outputId": "55e79d5d-663f-4717-afde-8d59409f46ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards\n",
      "goal (68, 6)\n",
      "obstacles [(21, 56), (74, 42), (50, 94), (52, 63), (68, 88), (85, 73), (21, 96), (45, 17), (47, 64), (67, 83), (47, 34), (28, 86), (74, 29), (76, 85), (68, 34), (72, 28), (54, 48), (27, 22), (55, 51), (53, 68), (48, 78), (46, 90), (53, 98), (29, 59), (56, 22), (84, 88), (45, 55), (33, 56), (82, 27), (77, 67), (65, 30), (60, 62), (64, 93), (23, 5), (1, 95), (12, 56), (39, 53), (71, 48), (53, 69), (23, 74), (67, 70), (54, 80), (30, 34), (19, 23), (51, 38), (2, 5), (50, 74), (53, 85), (39, 51), (16, 11), (83, 78), (33, 87), (68, 70), (55, 56), (48, 7), (70, 62), (85, 5), (26, 46), (64, 89), (30, 90), (99, 25), (60, 46), (36, 37), (72, 32), (86, 18), (37, 94), (66, 86), (24, 60), (77, 10), (47, 93), (26, 14), (86, 6), (78, 2), (98, 89), (11, 60), (91, 10), (18, 82), (21, 72), (0, 70), (81, 32), (21, 78), (26, 75), (86, 30), (74, 80), (54, 24), (31, 90), (14, 63), (35, 64), (19, 5), (19, 48), (82, 37), (98, 68), (1, 14), (84, 28), (20, 58), (55, 79), (61, 75), (64, 38), (98, 47), (48, 25)]\n",
      "\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# how long it takes to converge\n",
    "\n",
    "rows, cols = 100, 100\n",
    "\n",
    "rewards, goal, obs = generateRandomRewardsWithObstacles(rows, cols, 100, 0, -1000, 100)\n",
    "print (\"rewards\")\n",
    "# printMatrix(rewards)\n",
    "print (\"goal\", goal)\n",
    "print (\"obstacles\", obs)\n",
    "print()\n",
    "\n",
    "mdp = DeterministicGridWorldMDP(rewards, rows, cols)\n",
    "mdp.iterateValuesUntilConverge()\n",
    "policy1 = mdp.getPolicy()\n",
    "# printMatrix(markGoalAndObstaclesOnGrid(mdp.getPolicy(), goal, obs))\n",
    "\n",
    "mdp = DeterministicGridWorldMDP(rewards, rows, cols)\n",
    "mdp.repeatIterateValues(10)\n",
    "policy2 = mdp.getPolicy()\n",
    "# printMatrix(markGoalAndObstaclesOnGrid(mdp.getPolicy(), goal, obs))\n",
    "\n",
    "mdp = DeterministicGridWorldMDP(rewards, rows, cols)\n",
    "mdp.repeatIterateValues(50)\n",
    "policy3 = mdp.getPolicy()\n",
    "\n",
    "mdp = DeterministicGridWorldMDP(rewards, rows, cols)\n",
    "mdp.repeatIterateValues(100)\n",
    "policy4 = mdp.getPolicy()\n",
    "\n",
    "print(policy1 == policy2)\n",
    "print(policy2 == policy3)\n",
    "print(policy3 == policy4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "RLValueAndPolicyIterationGridWorld.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
