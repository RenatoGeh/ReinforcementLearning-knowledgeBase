{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipython-cache in /home/renato/anaconda3/envs/python36/lib/python3.6/site-packages (0.2.6)\n",
      "Requirement already satisfied: IPython in /home/renato/anaconda3/envs/python36/lib/python3.6/site-packages (from ipython-cache) (7.3.0)\n",
      "Requirement already satisfied: tabulate in /home/renato/anaconda3/envs/python36/lib/python3.6/site-packages (from ipython-cache) (0.8.3)\n",
      "Requirement already satisfied: astunparse in /home/renato/anaconda3/envs/python36/lib/python3.6/site-packages (from ipython-cache) (1.6.2)\n",
      "Requirement already satisfied: decorator in /home/renato/anaconda3/envs/python36/lib/python3.6/site-packages (from IPython->ipython-cache) (4.3.2)\n",
      "Requirement already satisfied: setuptools>=18.5 in /home/renato/anaconda3/envs/python36/lib/python3.6/site-packages (from IPython->ipython-cache) (40.8.0)\n",
      "Requirement already satisfied: traitlets>=4.2 in /home/renato/anaconda3/envs/python36/lib/python3.6/site-packages (from IPython->ipython-cache) (4.3.2)\n",
      "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /home/renato/anaconda3/envs/python36/lib/python3.6/site-packages (from IPython->ipython-cache) (2.0.9)\n",
      "Requirement already satisfied: pickleshare in /home/renato/anaconda3/envs/python36/lib/python3.6/site-packages (from IPython->ipython-cache) (0.7.5)\n",
      "Requirement already satisfied: jedi>=0.10 in /home/renato/anaconda3/envs/python36/lib/python3.6/site-packages (from IPython->ipython-cache) (0.13.3)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /home/renato/anaconda3/envs/python36/lib/python3.6/site-packages (from IPython->ipython-cache) (4.6.0)\n",
      "Requirement already satisfied: pygments in /home/renato/anaconda3/envs/python36/lib/python3.6/site-packages (from IPython->ipython-cache) (2.3.1)\n",
      "Requirement already satisfied: backcall in /home/renato/anaconda3/envs/python36/lib/python3.6/site-packages (from IPython->ipython-cache) (0.1.0)\n",
      "Requirement already satisfied: six<2.0,>=1.6.1 in /home/renato/anaconda3/envs/python36/lib/python3.6/site-packages (from astunparse->ipython-cache) (1.12.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/renato/anaconda3/envs/python36/lib/python3.6/site-packages (from astunparse->ipython-cache) (0.32.3)\n",
      "Requirement already satisfied: ipython-genutils in /home/renato/anaconda3/envs/python36/lib/python3.6/site-packages (from traitlets>=4.2->IPython->ipython-cache) (0.2.0)\n",
      "Requirement already satisfied: wcwidth in /home/renato/anaconda3/envs/python36/lib/python3.6/site-packages (from prompt-toolkit<2.1.0,>=2.0.0->IPython->ipython-cache) (0.1.7)\n",
      "Requirement already satisfied: parso>=0.3.0 in /home/renato/anaconda3/envs/python36/lib/python3.6/site-packages (from jedi>=0.10->IPython->ipython-cache) (0.3.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/renato/anaconda3/envs/python36/lib/python3.6/site-packages (from pexpect; sys_platform != \"win32\"->IPython->ipython-cache) (0.6.0)\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "\n",
    "!pip install ipython-cache\n",
    "import cache_magic\n",
    "from random import randint\n",
    "import sys\n",
    "import math\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7ACJ4WcqAdRg"
   },
   "source": [
    "# MDP Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9iTAO20cOV7z"
   },
   "outputs": [],
   "source": [
    "# Creating MDP representation for the grid world and organizing its functionalities\n",
    "\n",
    "class ActionResult:\n",
    "    def __init__ (self, resultState, prob, reward):\n",
    "        self.resultState = resultState\n",
    "        self.prob = prob\n",
    "        self.reward = reward\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"resultstate: {0}\\nprobability: {1}\\nReward: {2}\".format(str(self.resultState), str(self.prob), str(self.reward))\n",
    "\n",
    "class Action:\n",
    "    # abstraction for an action. Expects a state on which this action\n",
    "    # can be taken and a list of ActionResult\n",
    "    def __init__ (self, state, results, label):\n",
    "        self.state = state\n",
    "        self.results = results\n",
    "        self.label = label\n",
    "    \n",
    "    def __str__(self):\n",
    "        s = \"{0} - {1}\\n\".format(str(self.state), self.label)\n",
    "        for r in self.results:\n",
    "            s = s + str(r) + \"\\n\"\n",
    "            \n",
    "        return s\n",
    "#         return str(self.state) + \" \" + str([str(r) for r in self.results])\n",
    "     \n",
    "class State:\n",
    "    def __init__(self, id, actions, value):\n",
    "        self.id = id\n",
    "        self.actions = actions\n",
    "        self.value = value       \n",
    "\n",
    "        def __str__(self):\n",
    "            return str(self.id) + \" \" + str(self.value) + \" \" + str(self.actions)\n",
    "\n",
    "class GridWorldMDP(object):        \n",
    "    def __init__ (self, rewards, rows, cols, actions):\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "        self.rewards = rewards\n",
    "        self.policy = [['u' for j in range(cols)] for i in range(rows)]\n",
    "        self.value = [[0 for j in range(cols)] for i in range(rows)]\n",
    "        self.states = [[State((i, j), actions[(i, j)], 0) for j in range(cols)] for i in range(rows)]\n",
    "    \n",
    "    def getStateValue(self, state):\n",
    "        return self.states[state[0]][state[1]].value\n",
    "\n",
    "    def setStateValue(self, state, v):\n",
    "        self.states[state[0]][state[1]].value = v\n",
    "        \n",
    "    def upadateStateValue(self, state):\n",
    "        maxV = float(\"-inf\")\n",
    "        actions = self.states[state[0]][state[1]].actions\n",
    "        for i in range(len(actions)):\n",
    "            action = actions[i]\n",
    "            resultState = action.results[0].resultState\n",
    "            r = action.results[0].reward\n",
    "            v = self.getStateValue(resultState)\n",
    "            if r + v > maxV:\n",
    "                maxV = r + v\n",
    "                self.policy[state[0]][state[1]] = action.label\n",
    "                self.setStateValue(state, v)\n",
    "    \n",
    "    def printValues(self):\n",
    "        for r in self.value:\n",
    "            print(r)\n",
    "            \n",
    "    def getPolicy(self):\n",
    "        return self.policy\n",
    "        \n",
    "        \n",
    "    def printPolicy(self):\n",
    "        for r in self.policy:\n",
    "            print(r)\n",
    "        print()\n",
    "\n",
    "    def iterateValues(self):\n",
    "        for i in range(self.rows):\n",
    "            for j in range(self.cols):\n",
    "                self.upadateStateValue((i, j))\n",
    "\n",
    "    def iterateValuesUntilConverge(self, printSteps=False):\n",
    "        while True:\n",
    "            values = self.value\n",
    "            self.iterateValues()\n",
    "            if printSteps:\n",
    "                print()\n",
    "                self.printPolicy()      \n",
    "            if values == self.value:\n",
    "                break\n",
    "                \n",
    "    def repeatIterateValues(self, n, printSteps=False):\n",
    "        for i in range(n):\n",
    "            self.iterateValues()\n",
    "            if printSteps:\n",
    "                print()\n",
    "                self.printPolicy()        \n",
    "\n",
    "class DeterministicGridWorldMDP(GridWorldMDP):\n",
    "    transitions = [(0, 1), (0, -1), (1, 0), (-1, 0)]\n",
    "    nTransitionsPerState = len(transitions)\n",
    "    # creates a gridworld representation where the reward for every transition\n",
    "    # is given in the rewards matrix which means the reward the agent gets when achieving that reward\n",
    "    def __init__ (self, rewards, rows, cols):\n",
    "        actions = DeterministicGridWorldMDP.getDeterministicGridWorldDefaultActions(rewards, rows, cols)\n",
    "        super(DeterministicGridWorldMDP, self).__init__(rewards, rows, cols, actions)\n",
    "\n",
    "    @staticmethod\n",
    "    def getDeterministicGridWorldDefaultActions(rewards, rows, cols):\n",
    "        keys = []\n",
    "        for j in range(cols):\n",
    "            for i in range(rows):\n",
    "                keys.append((i, j))\n",
    "        actions = {key: DeterministicGridWorldMDP.generateActions(key[0], key[1], rewards, rows, cols) for key in keys}\n",
    "        \n",
    "        return actions\n",
    "        \n",
    "    @staticmethod\n",
    "    def isValidTransition(dest, rows, cols):\n",
    "        return  not ((dest[0] < 0 or dest[0] >= rows) or (dest[1] < 0 or dest[1] >= cols))\n",
    "    \n",
    "    @staticmethod\n",
    "    def generateActions(i, j, rewards, rows, cols):\n",
    "        actions = []\n",
    "        prettyDirs = ['r', 'l', 'd', 'u']\n",
    "        for transition in range(DeterministicGridWorldMDP.nTransitionsPerState):\n",
    "            resultState = (i + DeterministicGridWorldMDP.transitions[transition][0], j + DeterministicGridWorldMDP.transitions[transition][1])\n",
    "            if DeterministicGridWorldMDP.isValidTransition(resultState, rows, cols):\n",
    "                actions.append(Action((i, j), [ActionResult(resultState, 1, rewards[resultState[0]][resultState[1]])], prettyDirs[transition]))\n",
    "         \n",
    "        return actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Q0jChHJAwYW"
   },
   "source": [
    "# Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vrzTI84pA091"
   },
   "outputs": [],
   "source": [
    "def printMatrix(matrix):\n",
    "    for r in matrix:\n",
    "        print(r)\n",
    "    print()\n",
    "\n",
    "def generateRandomRewards(row, cols, goalReward, notGoalReward):\n",
    "    goalCoordX, goalCoordY = randint(0, rows - 1), randint(0, cols - 1)\n",
    "    rewards = [[notGoalReward for j in range(cols)] for i in range(rows)]\n",
    "    rewards[goalCoordX][goalCoordY] = goalReward\n",
    "    return rewards, (goalCoordX, goalCoordY)\n",
    "\n",
    "def generateRandomRewardsWithObstacles(row, cols, goalReward, notGoalReward, obstacleReward, nObstacles):\n",
    "    \n",
    "    rewards = [[notGoalReward for j in range(cols)] for i in range(rows)]\n",
    "    \n",
    "    goalCoordX, goalCoordY = randint(0, rows - 1), randint(0, cols - 1)\n",
    "    goal = (goalCoordX, goalCoordY)\n",
    "    rewards[goalCoordX][goalCoordY] = goalReward\n",
    "    \n",
    "    obstacles = []\n",
    "    while nObstacles > 0:\n",
    "        obs = randint(0, rows - 1), randint(0, cols - 1)\n",
    "        if not obs in obstacles and not obs == goal:\n",
    "            obstacles.append(obs)\n",
    "            nObstacles = nObstacles - 1\n",
    "            rewards[obs[0]][obs[1]] = obstacleReward\n",
    "    \n",
    "    return rewards, goal, obstacles\n",
    "\n",
    "\n",
    "def markGoalAndObstaclesOnGrid(grid, goal, obstacles):\n",
    "    grid[goal[0]][goal[1]] = 'G'\n",
    "    for o in obstacles:\n",
    "        grid[o[0]][o[1]] = 'X'\n",
    "    \n",
    "    return grid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "evLPPBUGAmRU"
   },
   "source": [
    "# Tests with deterministics gridworld environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2414
    },
    "colab_type": "code",
    "id": "SL4ajJPnOhZU",
    "outputId": "5dbdce19-efd8-4058-e620-13c0072a9a25",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['u', 'u', 'u']\n",
      "['u', 'u', 'u']\n",
      "['u', 'u', 'u']\n",
      "\n",
      "\n",
      "['r', 'd', 'l']\n",
      "['r', 'G', 'l']\n",
      "['r', 'u', 'l']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First dummy smoke test to make sure everything is not absurdly wrong\n",
    "\n",
    "rewards = [\n",
    "    [-1, -1, -1],\n",
    "    [-1, 100, -1],\n",
    "    [-1, -1, -1]\n",
    "]\n",
    "\n",
    "rows, cols = 3, 3\n",
    "\n",
    "mdp = DeterministicGridWorldMDP(rewards, rows, cols)\n",
    "mdp.printPolicy()\n",
    "mdp.repeatIterateValues(10)\n",
    "print ()\n",
    "printMatrix(markGoalAndObstaclesOnGrid(mdp.getPolicy(), (1,1), []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2309
    },
    "colab_type": "code",
    "id": "LXzCBKj-O0pi",
    "outputId": "b109b700-c9eb-4c3e-d434-caf18acd7920"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u']\n",
      "['u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u']\n",
      "['u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u']\n",
      "['u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u']\n",
      "['u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u']\n",
      "['u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u']\n",
      "['u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u']\n",
      "['u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u']\n",
      "['u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u']\n",
      "['u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u']\n",
      "\n",
      "['r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'l']\n",
      "['r', 'd', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'l']\n",
      "['r', 'G', 'l', 'r', 'r', 'r', 'r', 'r', 'r', 'l']\n",
      "['r', 'u', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'l']\n",
      "['r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'l']\n",
      "['r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'l']\n",
      "['r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'l']\n",
      "['r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'l']\n",
      "['r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'l']\n",
      "['r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'l']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Testing random test case for 10 rows and cols  \n",
    "\n",
    "rows, cols = 10, 10\n",
    "\n",
    "rewards, goal = generateRandomRewards(rows, cols, 100, 0)\n",
    "mdp = DeterministicGridWorldMDP(rewards, rows, cols)\n",
    "mdp.printPolicy()\n",
    "mdp.repeatIterateValues(5)\n",
    "printMatrix(markGoalAndObstaclesOnGrid(mdp.getPolicy(), goal, []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 833
    },
    "colab_type": "code",
    "id": "3Dyfhj1iRlY5",
    "outputId": "c633b0ff-eb8b-44e8-a5da-9a3f3382a9c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards\n",
      "[0, 0, 0, 0, -1000]\n",
      "[100, -1000, -1000, -1000, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[-1000, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "\n",
      "goal (1, 0)\n",
      "obstacles [(0, 4), (1, 2), (1, 3), (1, 1), (3, 0)]\n",
      "\n",
      "['u', 'u', 'u', 'u', 'u']\n",
      "['u', 'u', 'u', 'u', 'u']\n",
      "['u', 'u', 'u', 'u', 'u']\n",
      "['u', 'u', 'u', 'u', 'u']\n",
      "['u', 'u', 'u', 'u', 'u']\n",
      "\n",
      "\n",
      "['d', 'r', 'r', 'l', 'X']\n",
      "['G', 'X', 'X', 'X', 'd']\n",
      "['u', 'r', 'r', 'r', 'l']\n",
      "['X', 'r', 'r', 'r', 'l']\n",
      "['r', 'r', 'r', 'r', 'l']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Grid world with obstacles (things are getting interesting)\n",
    "\n",
    "rows, cols = 5, 5\n",
    "\n",
    "rewards, goal, obs = generateRandomRewardsWithObstacles(rows, cols, 100, 0, -1000, 5)\n",
    "print (\"rewards\")\n",
    "printMatrix(rewards)\n",
    "print (\"goal\", goal)\n",
    "print (\"obstacles\", obs)\n",
    "print()\n",
    "\n",
    "mdp = DeterministicGridWorldMDP(rewards, rows, cols)\n",
    "mdp.printPolicy()\n",
    "mdp.repeatIterateValues(10)\n",
    "print()\n",
    "\n",
    "printMatrix(markGoalAndObstaclesOnGrid(mdp.getPolicy(), goal, obs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 5175
    },
    "colab_type": "code",
    "id": "FYJZ2xzpaUYw",
    "outputId": "39c2bbaa-1253-45d7-8972-fd468744b7f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards\n",
      "[0, 0, 0, -1000, 0]\n",
      "[0, -1000, 0, 0, -1000]\n",
      "[-1000, 0, 0, 0, 0]\n",
      "[0, -1000, 100, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "\n",
      "goal (3, 2)\n",
      "obstacles [(1, 4), (3, 1), (2, 0), (1, 1), (0, 3)]\n",
      "\n",
      "['r', 'r', 'l', 'X', 'l']\n",
      "['u', 'X', 'r', 'l', 'X']\n",
      "['X', 'r', 'd', 'r', 'l']\n",
      "['d', 'X', 'G', 'l', 'l']\n",
      "['r', 'r', 'u', 'r', 'l']\n",
      "\n",
      "['r', 'r', 'l', 'X', 'l']\n",
      "['u', 'X', 'r', 'l', 'X']\n",
      "['X', 'r', 'd', 'r', 'l']\n",
      "['d', 'X', 'G', 'l', 'l']\n",
      "['r', 'r', 'u', 'r', 'l']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This case proves my iterate until converge is not working propperly\n",
    "\n",
    "rewards = [\n",
    "    [0, 0, 0, -1000, 0],\n",
    "    [0, -1000, 0, 0, -1000],\n",
    "    [-1000, 0, 0, 0, 0],\n",
    "    [0, -1000, 100, 0, 0],\n",
    "    [0, 0, 0, 0, 0]\n",
    "]\n",
    "\n",
    "goal = (3, 2)\n",
    "obs = [(1, 4), (3, 1), (2, 0), (1, 1), (0, 3)]\n",
    "print (\"rewards\")\n",
    "printMatrix(rewards)\n",
    "print (\"goal\", goal)\n",
    "print (\"obstacles\", obs)\n",
    "print()\n",
    "\n",
    "mdp = DeterministicGridWorldMDP(rewards, rows, cols)\n",
    "mdp.iterateValuesUntilConverge()\n",
    "\n",
    "printMatrix(markGoalAndObstaclesOnGrid(mdp.getPolicy(), goal, obs))\n",
    "\n",
    "mdp = DeterministicGridWorldMDP(rewards, rows, cols)\n",
    "mdp.repeatIterateValues(10)\n",
    "printMatrix(markGoalAndObstaclesOnGrid(mdp.getPolicy(), goal, obs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 5175
    },
    "colab_type": "code",
    "id": "41V_R0MyZfEO",
    "outputId": "e342fdf7-a7c3-4baa-92e5-e5bd4d81d84c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, -1000, 0, -1000]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, -1000, -1000, -1000, -1000]\n",
      "[0, 0, 0, 0, 100]\n",
      "\n",
      "goal (4, 4)\n",
      "obstacles [(3, 1), (3, 2), (3, 3), (3, 4), (1, 2), (1, 4)]\n",
      "\n",
      "['r', 'r', 'r', 'r', 'l']\n",
      "['r', 'l', 'X', 'd', 'X']\n",
      "['r', 'r', 'r', 'r', 'l']\n",
      "['d', 'X', 'X', 'X', 'X']\n",
      "['r', 'r', 'r', 'r', 'G']\n",
      "\n",
      "['r', 'r', 'r', 'r', 'l']\n",
      "['r', 'l', 'X', 'd', 'X']\n",
      "['r', 'r', 'r', 'r', 'l']\n",
      "['d', 'X', 'X', 'X', 'X']\n",
      "['r', 'r', 'r', 'r', 'G']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Another case where iterating until converge do not work\n",
    "\n",
    "rewards = [\n",
    "    [0,     0,      0,     0, 0],\n",
    "    [0,     0,     -1000,  0, -1000],\n",
    "    [0,     0,     0,      0, 0],\n",
    "    [0,     -1000, -1000,  -1000, -1000],\n",
    "    [0,     0,     0,    0, 100]\n",
    "]\n",
    "\n",
    "rows, cols = 5, 5\n",
    "\n",
    "goal = (4, 4)\n",
    "obs = [(3, 1),(3, 2), (3, 3), (3, 4), (1, 2), (1, 4)]\n",
    "print (\"rewards\")\n",
    "printMatrix(rewards)\n",
    "print (\"goal\", goal)\n",
    "print (\"obstacles\", obs)\n",
    "print()\n",
    "\n",
    "mdp = DeterministicGridWorldMDP(rewards, rows, cols)\n",
    "mdp.iterateValuesUntilConverge()\n",
    "printMatrix(markGoalAndObstaclesOnGrid(mdp.getPolicy(), goal, obs))\n",
    "\n",
    "mdp = DeterministicGridWorldMDP(rewards, rows, cols)\n",
    "mdp.repeatIterateValues(10)\n",
    "printMatrix(markGoalAndObstaclesOnGrid(mdp.getPolicy(), goal, obs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 5175
    },
    "colab_type": "code",
    "id": "9ltOyA_ef6vL",
    "outputId": "b182074a-6cc5-4a17-8c59-2eae7a183bba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, -1000, 0, -1000]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, -250, -500, -750, -1000]\n",
      "[0, 0, 0, 0, 100]\n",
      "\n",
      "goal (4, 4)\n",
      "obstacles [(3, 1), (3, 2), (3, 3), (3, 4), (1, 2), (1, 4)]\n",
      "\n",
      "['r', 'r', 'r', 'r', 'l']\n",
      "['r', 'l', 'X', 'd', 'X']\n",
      "['r', 'r', 'r', 'r', 'l']\n",
      "['d', 'X', 'X', 'X', 'X']\n",
      "['r', 'r', 'r', 'r', 'G']\n",
      "\n",
      "['r', 'r', 'r', 'r', 'l']\n",
      "['r', 'l', 'X', 'd', 'X']\n",
      "['r', 'r', 'r', 'r', 'l']\n",
      "['d', 'X', 'X', 'X', 'X']\n",
      "['r', 'r', 'r', 'r', 'G']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lets have variable obstacles with variable penalties\n",
    "\n",
    "rewards = [\n",
    "    [0,     0,      0,     0, 0],\n",
    "    [0,     0,     -1000,  0, -1000],\n",
    "    [0,     0,     0,      0, 0],\n",
    "    [0,     -250, -500,  -750, -1000],\n",
    "    [0,     0,     0,    0, 100]\n",
    "]\n",
    "\n",
    "rows, cols = 5, 5\n",
    "\n",
    "goal = (4, 4)\n",
    "obs = [(3, 1),(3, 2), (3, 3), (3, 4), (1, 2), (1, 4)]\n",
    "print (\"rewards\")\n",
    "printMatrix(rewards)\n",
    "print (\"goal\", goal)\n",
    "print (\"obstacles\", obs)\n",
    "print()\n",
    "\n",
    "mdp = DeterministicGridWorldMDP(rewards, rows, cols)\n",
    "mdp.iterateValuesUntilConverge()\n",
    "printMatrix(markGoalAndObstaclesOnGrid(mdp.getPolicy(), goal, obs))\n",
    "\n",
    "mdp = DeterministicGridWorldMDP(rewards, rows, cols)\n",
    "mdp.repeatIterateValues(10)\n",
    "printMatrix(markGoalAndObstaclesOnGrid(mdp.getPolicy(), goal, obs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 9517
    },
    "colab_type": "code",
    "id": "J_rzSqXrgxKa",
    "outputId": "0e74447f-8465-48b7-ae49-8621d08b7eaf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards\n",
      "[-100, -100, -100, -100, -10]\n",
      "[-100, -100, -1000, -100, -1000]\n",
      "[-100, -100, -100, -100, -10]\n",
      "[-100, -250, -500, -750, -1000]\n",
      "[-100, -100, -100, -100, 100]\n",
      "\n",
      "goal (4, 4)\n",
      "obstacles [(3, 1), (3, 2), (3, 3), (3, 4), (1, 2), (1, 4)]\n",
      "\n",
      "['r', 'r', 'r', 'r', 'l']\n",
      "['r', 'l', 'X', 'd', 'X']\n",
      "['r', 'r', 'r', 'r', 'l']\n",
      "['d', 'X', 'X', 'X', 'X']\n",
      "['r', 'r', 'r', 'r', 'G']\n",
      "\n",
      "['r', 'r', 'r', 'r', 'l']\n",
      "['r', 'l', 'X', 'd', 'X']\n",
      "['r', 'r', 'r', 'r', 'l']\n",
      "['d', 'X', 'X', 'X', 'X']\n",
      "['r', 'r', 'r', 'r', 'G']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lets now give negative reward for moving\n",
    "\n",
    "rewards = [\n",
    "    [-100,     -100,      -100,     -100, -10],\n",
    "    [-100,     -100,     -1000,  -100, -1000],\n",
    "    [-100,     -100,     -100,      -100, -10],\n",
    "    [-100,     -250, -500,  -750, -1000],\n",
    "    [-100,     -100,     -100,    -100, 100]\n",
    "]\n",
    "\n",
    "rows, cols = 5, 5\n",
    "\n",
    "goal = (4, 4)\n",
    "obs = [(3, 1),(3, 2), (3, 3), (3, 4), (1, 2), (1, 4)]\n",
    "print (\"rewards\")\n",
    "printMatrix(rewards)\n",
    "print (\"goal\", goal)\n",
    "print (\"obstacles\", obs)\n",
    "print()\n",
    "\n",
    "mdp = DeterministicGridWorldMDP(rewards, rows, cols)\n",
    "mdp.iterateValuesUntilConverge()\n",
    "printMatrix(markGoalAndObstaclesOnGrid(mdp.getPolicy(), goal, obs))\n",
    "\n",
    "mdp = DeterministicGridWorldMDP(rewards, rows, cols)\n",
    "mdp.repeatIterateValues(20)\n",
    "printMatrix(markGoalAndObstaclesOnGrid(mdp.getPolicy(), goal, obs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 4889258
    },
    "colab_type": "code",
    "id": "Fexcl8xdd7LW",
    "outputId": "55e79d5d-663f-4717-afde-8d59409f46ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards\n",
      "goal (34, 49)\n",
      "obstacles [(24, 48), (91, 15), (77, 84), (92, 57), (22, 10), (71, 2), (75, 21), (38, 47), (7, 73), (21, 38), (31, 89), (59, 68), (28, 9), (23, 98), (74, 98), (45, 55), (5, 29), (64, 29), (1, 28), (96, 77), (41, 45), (3, 55), (53, 9), (62, 62), (99, 75), (74, 65), (31, 52), (81, 10), (66, 52), (10, 78), (52, 68), (80, 82), (9, 93), (80, 8), (68, 17), (8, 64), (59, 8), (66, 41), (74, 82), (39, 7), (60, 10), (31, 72), (23, 68), (19, 63), (97, 59), (95, 35), (33, 87), (99, 66), (24, 14), (31, 10), (0, 69), (27, 6), (98, 49), (93, 65), (79, 82), (71, 78), (72, 2), (94, 68), (88, 30), (12, 26), (36, 25), (23, 2), (6, 34), (55, 8), (38, 76), (91, 71), (55, 26), (51, 17), (64, 44), (48, 58), (77, 8), (82, 50), (91, 49), (48, 29), (7, 40), (58, 80), (46, 25), (8, 62), (88, 40), (16, 63), (73, 2), (1, 14), (54, 51), (90, 77), (97, 77), (14, 97), (87, 96), (90, 38), (12, 24), (31, 43), (74, 22), (29, 65), (98, 46), (41, 53), (81, 27), (99, 39), (88, 37), (22, 41), (59, 55), (85, 16)]\n",
      "\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# how long it takes to converge\n",
    "\n",
    "rows, cols = 100, 100\n",
    "\n",
    "rewards, goal, obs = generateRandomRewardsWithObstacles(rows, cols, 100, 0, -1000, 100)\n",
    "print (\"rewards\")\n",
    "# printMatrix(rewards)\n",
    "print (\"goal\", goal)\n",
    "print (\"obstacles\", obs)\n",
    "print()\n",
    "\n",
    "mdp = DeterministicGridWorldMDP(rewards, rows, cols)\n",
    "mdp.iterateValuesUntilConverge()\n",
    "policy1 = mdp.getPolicy()\n",
    "# printMatrix(markGoalAndObstaclesOnGrid(mdp.getPolicy(), goal, obs))\n",
    "\n",
    "mdp = DeterministicGridWorldMDP(rewards, rows, cols)\n",
    "mdp.repeatIterateValues(10)\n",
    "policy2 = mdp.getPolicy()\n",
    "# printMatrix(markGoalAndObstaclesOnGrid(mdp.getPolicy(), goal, obs))\n",
    "\n",
    "mdp = DeterministicGridWorldMDP(rewards, rows, cols)\n",
    "mdp.repeatIterateValues(50)\n",
    "policy3 = mdp.getPolicy()\n",
    "\n",
    "mdp = DeterministicGridWorldMDP(rewards, rows, cols)\n",
    "mdp.repeatIterateValues(100)\n",
    "policy4 = mdp.getPolicy()\n",
    "\n",
    "print(policy1 == policy2)\n",
    "print(policy2 == policy3)\n",
    "print(policy3 == policy4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0) - r\n",
      "resultstate: (0, 1)\n",
      "probability: 1\n",
      "Reward: -1\n",
      "\n",
      "(0, 0) - d\n",
      "resultstate: (1, 0)\n",
      "probability: 1\n",
      "Reward: -1\n",
      "\n",
      "(1, 0) - r\n",
      "resultstate: (1, 1)\n",
      "probability: 1\n",
      "Reward: 100\n",
      "\n",
      "(1, 0) - d\n",
      "resultstate: (2, 0)\n",
      "probability: 1\n",
      "Reward: -1\n",
      "\n",
      "(1, 0) - u\n",
      "resultstate: (0, 0)\n",
      "probability: 1\n",
      "Reward: -1\n",
      "\n",
      "(2, 0) - r\n",
      "resultstate: (2, 1)\n",
      "probability: 1\n",
      "Reward: -1\n",
      "\n",
      "(2, 0) - u\n",
      "resultstate: (1, 0)\n",
      "probability: 1\n",
      "Reward: -1\n",
      "\n",
      "(0, 1) - r\n",
      "resultstate: (0, 2)\n",
      "probability: 1\n",
      "Reward: -1\n",
      "\n",
      "(0, 1) - l\n",
      "resultstate: (0, 0)\n",
      "probability: 1\n",
      "Reward: -1\n",
      "\n",
      "(0, 1) - d\n",
      "resultstate: (1, 1)\n",
      "probability: 1\n",
      "Reward: 100\n",
      "\n",
      "(1, 1) - r\n",
      "resultstate: (1, 2)\n",
      "probability: 1\n",
      "Reward: -1\n",
      "\n",
      "(1, 1) - l\n",
      "resultstate: (1, 0)\n",
      "probability: 1\n",
      "Reward: -1\n",
      "\n",
      "(1, 1) - d\n",
      "resultstate: (2, 1)\n",
      "probability: 1\n",
      "Reward: -1\n",
      "\n",
      "(1, 1) - u\n",
      "resultstate: (0, 1)\n",
      "probability: 1\n",
      "Reward: -1\n",
      "\n",
      "(2, 1) - r\n",
      "resultstate: (2, 2)\n",
      "probability: 1\n",
      "Reward: -1\n",
      "\n",
      "(2, 1) - l\n",
      "resultstate: (2, 0)\n",
      "probability: 1\n",
      "Reward: -1\n",
      "\n",
      "(2, 1) - u\n",
      "resultstate: (1, 1)\n",
      "probability: 1\n",
      "Reward: 100\n",
      "\n",
      "(0, 2) - l\n",
      "resultstate: (0, 1)\n",
      "probability: 1\n",
      "Reward: -1\n",
      "\n",
      "(0, 2) - d\n",
      "resultstate: (1, 2)\n",
      "probability: 1\n",
      "Reward: -1\n",
      "\n",
      "(1, 2) - l\n",
      "resultstate: (1, 1)\n",
      "probability: 1\n",
      "Reward: 100\n",
      "\n",
      "(1, 2) - d\n",
      "resultstate: (2, 2)\n",
      "probability: 1\n",
      "Reward: -1\n",
      "\n",
      "(1, 2) - u\n",
      "resultstate: (0, 2)\n",
      "probability: 1\n",
      "Reward: -1\n",
      "\n",
      "(2, 2) - l\n",
      "resultstate: (2, 1)\n",
      "probability: 1\n",
      "Reward: -1\n",
      "\n",
      "(2, 2) - u\n",
      "resultstate: (1, 2)\n",
      "probability: 1\n",
      "Reward: -1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First dummy smoke test to make sure everything is not absurdly wrong\n",
    "\n",
    "rewards = [\n",
    "    [-1, -1, -1],\n",
    "    [-1, 100, -1],\n",
    "    [-1, -1, -1]\n",
    "]\n",
    "\n",
    "rows, cols = 3, 3\n",
    "\n",
    "stateActions = DeterministicGridWorldMDP.getDeterministicGridWorldDefaultActions(rewards, rows, cols)\n",
    "for s in stateActions:\n",
    "    for a in stateActions[s]:\n",
    "        print(str(a))\n",
    "\n",
    "# print(stateActions.__dict__)\n",
    "# s = json.dumps(stateActions)\n",
    "# print(s)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "RLValueAndPolicyIterationGridWorld.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
